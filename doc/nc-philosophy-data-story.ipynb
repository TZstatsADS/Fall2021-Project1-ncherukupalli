{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ebf477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca94783",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15787d95",
   "metadata": {},
   "source": [
    "First, let us import and briefly summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd39647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 360808\n",
      "Columns: ['title' 'author' 'school' 'sentence_spacy' 'sentence_str'\n",
      " 'original_publication_date' 'corpus_edition_date' 'sentence_length'\n",
      " 'sentence_lowered' 'tokenized_txt' 'lemmatized_str']\n",
      "# of Authors: 36\n",
      "# of Schools: 13\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_DATA = \"../data/raw_data.csv\"\n",
    "raw_df = pd.read_csv(PATH_TO_DATA)\n",
    "\n",
    "# Output meta data\n",
    "print(\"Length: {}\".format(len(raw_df)))\n",
    "print(\"Columns: {}\".format(raw_df.columns.values))\n",
    "print(\"# of Authors: {}\".format(len(raw_df[\"author\"].unique())))\n",
    "print(\"# of Schools: {}\".format(len(raw_df[\"school\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d5bba",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512ef59",
   "metadata": {},
   "source": [
    "Let us consider a subset of the data containing the columns: title, author, school, original_publication_date, corpus_edition_date, sentence_str, tokenized_txt.\n",
    "\n",
    "We should omit columns such as lemmatized_str. Instead, we should remove stop words and lemmetized the sentences ourselves to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2a988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = [\"title\", \"author\", \"school\",\n",
    "                 \"original_publication_date\", \"corpus_edition_date\",\n",
    "                 \"sentence_str\", \"tokenized_txt\"]\n",
    "\n",
    "df = raw_df.loc[:, relevant_cols].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5a8dc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from lib.build_features import get_clean_sentence, get_vectorized_sentences\n",
    "\n",
    "df[\"lemmatized_str\"] = df[\"tokenized_txt\"].apply(\n",
    "    lambda row: get_clean_sentence(ast.literal_eval(row))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e33ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Fall2021-Project1-ncherukupalli)",
   "language": "python",
   "name": "pycharm-d0f0d84a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}